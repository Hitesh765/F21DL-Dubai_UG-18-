Summary:
D1: Neural Networks
    1. Implemented an MLP with two hidden layers, using ReLU activation and the Adam optimizer.
    2. Standardized features to enhance convergence and ensure compatibility with neural network models.
    3. Trained and evaluated the MLP model to assess its ability to capture non-linear relationships in structured data.

D2: Neural Networks
    1. Implemented and trained MLP models with varying architectures (2, 3, and 4 hidden layers with 300 and 500 neurons).
    2. Built and trained the first CNN model, testing its performance against MLP baselines.
    3. Implemented a second CNN model with deeper architecture and more parameters.
    4. Compared CNN results, focusing on improved generalization, reduced loss, and higher accuracy compared to MLPs.

References:
H. Liu and H. Motoda, Feature Selection for Knowledge Discovery and Data Mining. Boston, MA: Springer, 1998. [Online]. Available: https://link.springer.com/book/10.1007/978-1-4615-5689-3

Challenges: 
Ensuring that the neural networks converged and selecting appropriate architectures posed significant challenges. Comparing the performance of MLPs and CNNs required extensive testing and validation.

Next Steps: 
Once the initial models are operational, proceed with hyperparameter tuning through grid search and random search techniques to enhance model efficiency and accuracy. Finalize the report and the GitHub Repositry and Submit the coursework before deadline.


