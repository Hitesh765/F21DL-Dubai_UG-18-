Summary:
D1: Traditional ML Algorithms
    1. Prepared the dataset by encoding and scaling features such as team interactions, venue statistics, and toss decisions.
    2. Trained k-NN, Decision Trees, and Random Forest models, iteratively tuning hyperparameters for optimal performance.
    3. Analyzed the models' ability to handle feature variability and established baseline results for further experimentation.

D2: Traditional ML Algorithms 
    1. Conducted detailed analysis of class-specific patterns using visualizations such as mean images and standard deviations to identify key attributes and intra-class variability.
    2. Compared Random Forestâ€™s ensemble performance against individual models, highlighting its stability and improved accuracy.
    3. Compiled insights from traditional model experiments to guide transitions toward advanced architectures like CNNs.

References:
C. M. Bishop, Pattern Recognition and Machine Learning. New York: Springer, 2006. [Online]. Available: https://www.cs.uoi.gr/~arly/courses/ml/tmp/Bishop_book.pdf

Challenges:
Hyperparameter tuning was particularly challenging due to the diverse nature of the data. Achieving high performance across different models required careful feature selection and model configuration.

Next Steps:
Continue to enhance the performance of traditional models and begin exploring neural network architectures, focusing on MLPs initially.

